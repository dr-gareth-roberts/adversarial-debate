name: 'Adversarial Debate Security Analysis'
description: 'AI Red Team Security Testing using adversarial agent mesh'
author: 'Gareth Roberts'

branding:
  icon: 'shield'
  color: 'red'

inputs:
  target:
    description: 'File or directory to analyze'
    required: false
    default: '.'

  agent:
    description: 'Single agent to run (exploit, break, chaos) or "all" for full pipeline'
    required: false
    default: 'all'

  provider:
    description: 'LLM provider (anthropic, openai, azure, ollama, mock)'
    required: false
    default: 'anthropic'

  anthropic-api-key:
    description: 'Anthropic API key (if using anthropic provider)'
    required: false

  openai-api-key:
    description: 'OpenAI API key (if using openai provider)'
    required: false

  azure-openai-key:
    description: 'Azure OpenAI API key (if using azure provider)'
    required: false

  azure-openai-endpoint:
    description: 'Azure OpenAI endpoint URL (if using azure provider)'
    required: false

  output-format:
    description: 'Output format (json, sarif, html, markdown)'
    required: false
    default: 'sarif'

  output-file:
    description: 'Output file path'
    required: false
    default: 'security-analysis.sarif'

  fail-on-findings:
    description: 'Fail the action if security findings are detected'
    required: false
    default: 'true'

  severity-threshold:
    description: 'Minimum severity to fail on (critical, high, medium, low)'
    required: false
    default: 'high'

  time-budget:
    description: 'Time budget in seconds for analysis'
    required: false
    default: '300'

  python-version:
    description: 'Python version to use'
    required: false
    default: '3.11'

outputs:
  findings-count:
    description: 'Total number of security findings'
    value: ${{ steps.analyze.outputs.findings-count }}

  critical-count:
    description: 'Number of critical severity findings'
    value: ${{ steps.analyze.outputs.critical-count }}

  high-count:
    description: 'Number of high severity findings'
    value: ${{ steps.analyze.outputs.high-count }}

  verdict:
    description: 'Overall verdict (BLOCK, WARN, PASS)'
    value: ${{ steps.analyze.outputs.verdict }}

  output-file:
    description: 'Path to the output file'
    value: ${{ steps.analyze.outputs.output-file }}

runs:
  using: 'composite'
  steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install adversarial-debate
      shell: bash
      run: |
        pip install adversarial-debate

    - name: Run security analysis
      id: analyze
      shell: bash
      env:
        ANTHROPIC_API_KEY: ${{ inputs.anthropic-api-key }}
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
        AZURE_OPENAI_API_KEY: ${{ inputs.azure-openai-key }}
        AZURE_OPENAI_ENDPOINT: ${{ inputs.azure-openai-endpoint }}
        LLM_PROVIDER: ${{ inputs.provider }}
      run: |
        # Determine command based on agent selection
        if [ "${{ inputs.agent }}" = "all" ]; then
          CMD="adversarial-debate run"
        else
          CMD="adversarial-debate analyze ${{ inputs.agent }}"
        fi

        # Run analysis
        OUTPUT_FILE="${{ inputs.output-file }}"
        $CMD "${{ inputs.target }}" \
          --output "$OUTPUT_FILE" \
          --time-budget ${{ inputs.time-budget }} \
          --json-output > results.json 2>&1 || true

        # Parse results
        if [ -f results.json ]; then
          FINDINGS=$(cat results.json | python3 -c "
        import json, sys
        try:
            data = json.load(sys.stdin)
            findings = data.get('findings', {})
            if isinstance(findings, dict):
                total = sum(findings.values())
            else:
                total = len(data.get('findings', []))
            print(total)
        except:
            print(0)
        ")

          CRITICAL=$(cat results.json | python3 -c "
        import json, sys
        try:
            data = json.load(sys.stdin)
            verdict = data.get('verdict', {}).get('summary', {})
            print(verdict.get('blocking_issues', 0))
        except:
            print(0)
        ")

          HIGH=$(cat results.json | python3 -c "
        import json, sys
        try:
            data = json.load(sys.stdin)
            verdict = data.get('verdict', {}).get('summary', {})
            print(verdict.get('warnings', 0))
        except:
            print(0)
        ")

          VERDICT=$(cat results.json | python3 -c "
        import json, sys
        try:
            data = json.load(sys.stdin)
            verdict = data.get('verdict', {}).get('summary', {})
            print(verdict.get('decision', 'UNKNOWN'))
        except:
            print('UNKNOWN')
        ")
        else
          FINDINGS=0
          CRITICAL=0
          HIGH=0
          VERDICT="UNKNOWN"
        fi

        echo "findings-count=$FINDINGS" >> $GITHUB_OUTPUT
        echo "critical-count=$CRITICAL" >> $GITHUB_OUTPUT
        echo "high-count=$HIGH" >> $GITHUB_OUTPUT
        echo "verdict=$VERDICT" >> $GITHUB_OUTPUT
        echo "output-file=$OUTPUT_FILE" >> $GITHUB_OUTPUT

        # Convert to requested format if needed
        if [ "${{ inputs.output-format }}" = "sarif" ] && [ -f results.json ]; then
          python3 -c "
        import json
        from adversarial_debate.formatters import get_formatter

        with open('results.json') as f:
            data = json.load(f)

        formatter = get_formatter('sarif')
        sarif = formatter.format(data)

        with open('$OUTPUT_FILE', 'w') as f:
            f.write(sarif)
        "
        fi

    - name: Upload SARIF to GitHub Security
      if: inputs.output-format == 'sarif' && github.event_name == 'push'
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: ${{ inputs.output-file }}
      continue-on-error: true

    - name: Check severity threshold
      if: inputs.fail-on-findings == 'true'
      shell: bash
      run: |
        VERDICT="${{ steps.analyze.outputs.verdict }}"
        THRESHOLD="${{ inputs.severity-threshold }}"
        CRITICAL="${{ steps.analyze.outputs.critical-count }}"
        HIGH="${{ steps.analyze.outputs.high-count }}"

        SHOULD_FAIL=false

        case "$THRESHOLD" in
          critical)
            if [ "$CRITICAL" -gt 0 ]; then
              SHOULD_FAIL=true
            fi
            ;;
          high)
            if [ "$CRITICAL" -gt 0 ] || [ "$HIGH" -gt 0 ]; then
              SHOULD_FAIL=true
            fi
            ;;
          medium|low)
            if [ "$VERDICT" = "BLOCK" ] || [ "$VERDICT" = "WARN" ]; then
              SHOULD_FAIL=true
            fi
            ;;
        esac

        if [ "$SHOULD_FAIL" = "true" ]; then
          echo "::error::Security findings exceed threshold ($THRESHOLD)"
          echo "Verdict: $VERDICT"
          echo "Critical: $CRITICAL, High: $HIGH"
          exit 1
        fi
