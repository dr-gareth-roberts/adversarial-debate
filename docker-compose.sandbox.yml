# Adversarial Debate - Sandboxed Execution Configuration
#
# This compose file provides fully isolated execution for adversarial debates.
# Use this when you want maximum isolation between the debate agents and your host system.
#
# Usage:
#   docker-compose -f docker-compose.yml -f docker-compose.sandbox.yml up debate
#
# This configuration:
#   - Runs debates in an isolated network
#   - Uses read-only filesystems where possible
#   - Limits container capabilities

services:
  # ==========================================================================
  # Fully isolated debate orchestrator
  # ==========================================================================
  debate:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        INSTALL_ALL_PROVIDERS: "true"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-anthropic}
      - ADVERSARIAL_LOG_LEVEL=${ADVERSARIAL_LOG_LEVEL:-INFO}
      - ADVERSARIAL_LOG_FORMAT=${ADVERSARIAL_LOG_FORMAT:-text}
      - ADVERSARIAL_OUTPUT_DIR=/output
      - ADVERSARIAL_BEAD_LEDGER=/output/beads/ledger.jsonl
    volumes:
      # Read-only access to code being analyzed
      - ${TARGET_CODE_PATH:-.}:/target:ro
      # Output directory for results
      - ./output:/output
    working_dir: /target
    command: ["run", "--time-budget", "${TIME_BUDGET:-300}", "/target"]
    networks:
      - debate_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=200M,mode=1777
    cap_drop:
      - ALL

  # ==========================================================================
  # Isolated debate with Ollama (local LLM - no API keys needed)
  # ==========================================================================
  debate-local:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - OLLAMA_BASE_URL=http://ollama-isolated:11434
      - LLM_PROVIDER=ollama
      - LLM_MODEL=${LLM_MODEL:-llama3.2}
      - ADVERSARIAL_LOG_LEVEL=${ADVERSARIAL_LOG_LEVEL:-INFO}
      - ADVERSARIAL_LOG_FORMAT=${ADVERSARIAL_LOG_FORMAT:-text}
      - ADVERSARIAL_OUTPUT_DIR=/output
      - ADVERSARIAL_BEAD_LEDGER=/output/beads/ledger.jsonl
    volumes:
      - ${TARGET_CODE_PATH:-.}:/target:ro
      - ./output:/output
    working_dir: /target
    command: ["run", "--time-budget", "${TIME_BUDGET:-300}", "/target"]
    networks:
      - debate_network
      - ollama_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=200M,mode=1777
    cap_drop:
      - ALL
    depends_on:
      ollama-isolated:
        condition: service_healthy
    profiles:
      - local

  # Isolated Ollama instance for local debates
  ollama-isolated:
    image: ollama/ollama:latest
    networks:
      - ollama_network
    volumes:
      - ollama_isolated_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    profiles:
      - local

  # ==========================================================================
  # Multi-agent debate with parallel execution
  # ==========================================================================
  debate-parallel:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        INSTALL_ALL_PROVIDERS: "true"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-anthropic}
      - PARALLEL_AGENTS=${PARALLEL_AGENTS:-3}
      - ADVERSARIAL_LOG_LEVEL=${ADVERSARIAL_LOG_LEVEL:-INFO}
      - ADVERSARIAL_LOG_FORMAT=${ADVERSARIAL_LOG_FORMAT:-text}
      - ADVERSARIAL_OUTPUT_DIR=/output
      - ADVERSARIAL_BEAD_LEDGER=/output/beads/ledger.jsonl
    volumes:
      - ${TARGET_CODE_PATH:-.}:/target:ro
      - ./output:/output
    working_dir: /target
    command: ["run", "--parallel", "${PARALLEL_AGENTS:-3}", "--time-budget", "${TIME_BUDGET:-300}", "/target"]
    networks:
      - debate_network
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:size=200M,mode=1777
    cap_drop:
      - ALL
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    profiles:
      - parallel

# ==========================================================================
# Networks
# ==========================================================================
networks:
  # Network for debate orchestration
  debate_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "true"

  # Isolated network for Ollama
  ollama_network:
    driver: bridge
    internal: true  # Only internal communication

# ==========================================================================
# Volumes
# ==========================================================================
volumes:
  ollama_isolated_data:
    driver: local
